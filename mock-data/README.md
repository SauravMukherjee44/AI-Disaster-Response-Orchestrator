# Mock Disaster Data System

Complete mock data ingestion system for testing, demos, and development.

## Overview

This system provides realistic disaster scenarios from multiple sources:
- **Official Weather Services**: NOAA, EFAS, IMD, BOM, etc.
- **Seismological Agencies**: USGS, JMA, EMSC, GeoNet
- **Social Media Platforms**: Twitter, Facebook, Instagram, TikTok, Reddit

## Data Files

### 1. flood-alerts.json
**5 realistic flood scenarios**

Locations covered:
- Houston, Texas (Dam failure emergency)
- Venice, Italy (High tide flooding)
- Sylhet, Bangladesh (Monsoon flooding)
- Brisbane, Australia (River flooding)
- Mumbai, India (Extreme rainfall)

**Severity distribution**: 2 critical, 3 high
**Total affected population**: ~9M people

### 2. earthquake-alerts.json
**6 major earthquake scenarios**

Locations covered:
- San Francisco, USA (M7.2 on San Andreas Fault)
- Fukushima, Japan (M6.8 offshore with tsunami)
- Southern Turkey (M7.5 catastrophic)
- Wellington, NZ (M5.6 moderate)
- Oaxaca, Mexico (M7.0 major)
- West Java, Indonesia (M5.8 shallow)

**Magnitude range**: M5.6 - M7.5
**Total affected population**: ~4M people

### 3. social-media-alerts.json
**10 social media emergency posts**

Platforms included:
- Twitter (4 posts)
- Facebook (2 posts)
- Instagram (1 post)
- TikTok (1 post)
- Reddit (1 post)
- WhatsApp (1 post)

**Features**: Real-time reports, user-generated content, varying verification levels

## Data Structure

Each disaster record includes:

```json
{
  "source": "Official agency or social media account",
  "timestamp": "ISO 8601 timestamp",
  "location": {
    "name": "Human-readable location",
    "latitude": 37.7749,
    "longitude": -122.4194
  },
  "severity": "critical|high|medium|low",
  "disaster_type": "earthquake|flood|fire|hurricane|tornado|tsunami",
  "message": "Detailed disaster description",
  "affected_population": 50000,
  "metadata": {
    "source-specific fields": "varies by source"
  }
}
```

## Usage Methods

### Method 1: CLI Script (Recommended for Development)

```bash
cd mock-data

# Ingest all data with 2-second delays
node ingest-data.js all 2000

# Ingest only floods with 1-second delay
node ingest-data.js floods 1000

# Ingest earthquakes with no delay (fast mode)
node ingest-data.js earthquakes 0

# Ingest social media posts
node ingest-data.js social 1000

# Show help
node ingest-data.js --help
```

**Environment Setup:**
```bash
export NEXT_PUBLIC_SUPABASE_URL="your-supabase-url"
export NEXT_PUBLIC_SUPABASE_ANON_KEY="your-anon-key"
```

### Method 2: API Endpoint

```bash
# Via curl
curl -X POST http://localhost:3000/api/ingest-mock-data \
  -H "Content-Type: application/json" \
  -d '{"category": "floods", "limit": 3}'

# Check available data
curl http://localhost:3000/api/ingest-mock-data
```

**API Parameters:**
- `category`: "all", "floods", "earthquakes", "social"
- `limit`: Number of records to ingest (optional)

**API Response:**
```json
{
  "success": true,
  "summary": {
    "total": 5,
    "successful": 5,
    "failed": 0,
    "success_rate": "100.0%"
  },
  "results": [...]
}
```

### Method 3: Dashboard UI (Best for Demos)

1. Open the dashboard in your browser
2. Find the "Mock Data Ingestion" panel
3. Select data source (All/Floods/Earthquakes/Social)
4. Choose number of records
5. Click "Ingest Data"
6. Watch real-time processing!

**Perfect for hackathon demos** - visually shows the system processing multiple disasters.

## Processing Pipeline

When you ingest mock data, each record goes through:

1. **Data Validation** â†’ Validates required fields
2. **Disaster Creation** â†’ Creates record in `disasters` table
3. **AI Summarization** â†’ Cline generates analysis (~500ms)
4. **RL Prioritization** â†’ Oumi calculates priority scores (~300ms)
5. **Real-time Broadcast** â†’ Updates dashboard instantly

**Total time per disaster**: ~1-2 seconds

## Demo Scenarios

### Scenario 1: Multi-Source Crisis
**Demonstrates**: System handling diverse data sources

```bash
node ingest-data.js all 1000
```

Shows floods, earthquakes, and social media posts being processed together.

### Scenario 2: Rapid Emergency Response
**Demonstrates**: High-volume ingestion

```bash
node ingest-data.js earthquakes 0
```

Processes 6 earthquakes as fast as possible to show scalability.

### Scenario 3: Social Media Integration
**Demonstrates**: Crowdsourced intelligence

```bash
node ingest-data.js social 2000
```

Ingests posts from Twitter, Facebook, Instagram, etc.

### Scenario 4: Single Critical Event
**Demonstrates**: Detailed AI analysis

```bash
# Use the UI to ingest 1 critical flood from Houston
```

Watch the AI generate detailed insights and recommendations.

## Metadata Examples

### Official Weather Service
```json
"metadata": {
  "water_level": "15.2 feet above flood stage",
  "rainfall_24h": "18 inches",
  "wind_speed": "35 mph",
  "alert_code": "FFW-TX-2024-0315",
  "evacuation_routes": ["I-10 East", "US-290 North"]
}
```

### Seismological Agency
```json
"metadata": {
  "magnitude": 7.2,
  "depth_km": 12,
  "fault_line": "San Andreas Fault",
  "intensity": "VIII - Severe shaking",
  "aftershock_probability": "95% within 24 hours",
  "tsunami_warning": true
}
```

### Social Media
```json
"metadata": {
  "platform": "Twitter",
  "engagement": 15000,
  "verified": true,
  "sentiment": "urgent",
  "keywords": ["buildings collapsed", "trapped", "fires"],
  "video_attached": true
}
```

## Customization

### Adding New Scenarios

1. Edit the appropriate JSON file (floods/earthquakes/social)
2. Follow the existing data structure
3. Ensure all required fields are present
4. Test with: `node ingest-data.js [category] 0`

**Example: Add a tsunami**
```json
{
  "source": "Pacific Tsunami Warning Center",
  "timestamp": "2024-03-15T20:00:00Z",
  "location": {
    "name": "Honolulu, Hawaii",
    "latitude": 21.3099,
    "longitude": -157.8581
  },
  "severity": "critical",
  "disaster_type": "tsunami",
  "message": "TSUNAMI WARNING: Waves 10-15 feet expected...",
  "affected_population": 350000,
  "metadata": {
    "wave_height_forecast": "10-15 feet",
    "time_to_arrival": "45 minutes"
  }
}
```

### Creating Custom Data Feeds

```javascript
// custom-feed.js
const customDisasters = [
  // your disaster objects
];

async function ingestCustom() {
  for (const disaster of customDisasters) {
    await fetch('http://localhost:3000/api/disasters', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(disaster)
    });
  }
}

ingestCustom();
```

## Kestra Integration

Use mock data in Kestra workflows:

```yaml
id: bulk-disaster-ingestion
namespace: disaster-response

tasks:
  - id: ingest-mock-floods
    type: io.kestra.plugin.core.http.Request
    uri: "http://localhost:3000/api/ingest-mock-data"
    method: POST
    body: |
      {
        "category": "floods",
        "limit": 5
      }

  - id: ingest-mock-earthquakes
    type: io.kestra.plugin.core.http.Request
    uri: "http://localhost:3000/api/ingest-mock-data"
    method: POST
    body: |
      {
        "category": "earthquakes",
        "limit": 5
      }
```

## Testing & Validation

### Verify Ingestion

```bash
# Check database
psql -c "SELECT COUNT(*) FROM disasters WHERE metadata->>'original_source' IS NOT NULL;"

# Via API
curl http://localhost:3000/api/disasters | jq '.disasters | length'
```

### Clear Mock Data

```sql
-- In Supabase SQL Editor
DELETE FROM disasters
WHERE metadata->>'original_source' IS NOT NULL;
```

Or via UI: Use the dashboard filters to identify and manage mock data.

## Performance Metrics

**Ingestion Speed:**
- CLI with delays: ~5 records/minute (demo-friendly)
- CLI without delays: ~30 records/minute (testing)
- API bulk: ~10 records/minute (parallel processing)

**System Processing:**
- Disaster creation: ~50ms
- AI summarization: ~500ms
- RL prioritization: ~300ms
- Total per record: ~850ms + network

**Dashboard Update:**
- Real-time latency: <100ms
- Visible to users: Instant

## Troubleshooting

### Issue: "Cannot find module @supabase/supabase-js"
```bash
cd mock-data
npm install @supabase/supabase-js
```

### Issue: "Error loading JSON file"
Check that you're running from the project root or mock-data directory.

### Issue: "API request failed"
- Verify dev server is running: `npm run dev`
- Check environment variables are set
- Confirm Supabase credentials are correct

### Issue: "No data appearing in dashboard"
- Refresh the page
- Check browser console for errors
- Verify WebSocket connection is active

## Best Practices

### For Development
- Use small batches (3-5 records) for quick testing
- Set delays to 0 for faster iteration
- Clear mock data between test runs

### For Demos
- Use delays of 1-2 seconds for visual effect
- Start with 1-2 records to show detail
- Use "All" category to show variety
- Have dashboard visible while ingesting

### For Production Testing
- Ingest full datasets to test scalability
- Monitor system resources during bulk loads
- Verify all AI and RL processing completes
- Check database performance under load

## Statistics

**Total Mock Disasters**: 21 scenarios
**Countries Represented**: 12
**Official Sources**: 10
**Social Media Sources**: 8
**Disaster Types**: 5 (flood, earthquake, fire, hurricane, tsunami)
**Severity Levels**: All 4 (critical, high, medium, low)
**Total Simulated Population Affected**: ~13 million people

## Contributing

To add more mock data:

1. Research real disaster events for authenticity
2. Follow the JSON schema exactly
3. Include realistic metadata
4. Test with the ingestion script
5. Document any new fields

## License

This mock data is for testing and development purposes only. Real disaster information should come from official sources.

---

**Ready to test?**

```bash
node ingest-data.js all 2000
```

Watch your dashboard come alive with realistic disaster scenarios! ðŸŒðŸš¨
